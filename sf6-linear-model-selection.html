<!DOCTYPE html>
<html lang="[zh]">
<head>
	<link rel="stylesheet" type="text/css" href="/theme/css/style.css">
	<!--<link rel="stylesheet/less" type="text/css" href="/theme/css/style.less">-->
	<!--<script src="/theme/js/less.js" type="text/javascript"></script>-->
	<link rel="stylesheet" type="text/css" href="/theme/css/pygments.css">
	<link rel="stylesheet" type="text/css" href="/theme/css/ydSetting.css">
	<link href='//fonts.googleapis.com/css?family=Open+Sans:800,400,300|Inconsolata' rel='stylesheet' type='text/css'>
  <link rel="shortcut icon" href="./ico/apple-touch-icon.png">
	<link href="aady5566.github.io/" type="application/atom+xml" rel="alternate" title="5566 ATOM Feed" />


		<title>5566</title>
		<meta charset="utf-8" />
</head>
<body>
	<section id="sidebar">
		<figure id="user_logo">
            <!-- <a href=""><div class="logo">&nbsp;</div></a> -->
						<img class="circular" src="https://raw.githubusercontent.com/aady5566/aady5566.github.io/master/pic/yd_fbas.jpg">
		</figure>
	</br>
		<div class="user_meta">
            <h1 id="user"><a href="http://aady5566.github.io" class="">YD</a></h1>
			<h2></h2>
			<p class="bio">lives in a cave.</p>
			<ul>
					<li><a href="http://aady5566.github.io/timeline/" target="_blank">timeline</a></li>
					<li><a href="https://tw.linkedin.com/in/yd-huang-735358aa" target="_blank">linkedin</a></li>
					<li><a href="https://github.com/aady5566" target="_blank">github</a></li>
					<li><a href="mailto:ydhuang@ntu.edu.tw" target="_blank">mail</a></li>
			</ul>
		</div>
		<footer>
			<address>
				&copy 2016 YD <br> Powered by Pelican.
			</address>
		</footer>
	</section>

	<section id="posts">
	<header>
		<h1>YD's blog</h1>
		<nav id="header-nav">
				<a id="header-nav-item"  href="/archives.html">Archives</a>
				<a id="header-nav-item"  href="/categories.html">Categories</a>
				<a id="header-nav-item"  href="/tags.html">Tags</a>
		</nav>
		<h3 id="date">Posted 三 22 3月 2017</h3>
	</header>
	<article>
		<h1 id="title">
			<a href="/sf6-linear-model-selection.html" rel="bookmark"
				title="Permalink to SF6 Linear Model Selection">SF6 Linear Model Selection</a>
		</h1>
		<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ['\\(','\\)'] ]
  }
});

(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-77164531-1', 'auto');
ga('send', 'pageview');
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/markdown-it/8.3.1/markdown-it.js">
</script>

<h1>Methods of model selection:</h1>
<p>今天要介紹模型的適選，主要有三種大方向：</p>
<ul>
<li>Subset selection</li>
<li>Shrinkage methods</li>
<li>Dimension reduction methods</li>
</ul>
<p>接下來就依照這三個方向進行介紹</p>
<h1>Subset selection</h1>
<h2>Best Subset Selection</h2>
<div class="img">
<img src="https://d2mxuefqeaa7sj.cloudfront.net/s_675D8EAEE35D72F3DF626C18BB07F441157F58865CAEC85F97D5D9051B1B596F_1486696508901_+2017-02-10+11.14.57.png
" width="80%" height="80%" />       
</div>

<ol>
<li>intercept model</li>
<li>training error</li>
<li>testing error (indirect and direct estimate)  </li>
</ol>
<blockquote>
<p>當預測因子大於40不建議用 best subset selection (迭代次數 $2^{40}$)</p>
</blockquote>
<hr />
<h2>Forward Stepwise Selection</h2>
<div class="img">
<img src="https://d2mxuefqeaa7sj.cloudfront.net/s_675D8EAEE35D72F3DF626C18BB07F441157F58865CAEC85F97D5D9051B1B596F_1486696930600_+2017-02-10+11.21.56.png
" width="80%" height="80%" />       
</div>

<blockquote>
<p>迭代次數相對較少，只有 $\frac{p^2+p}{2}$ 次 </p>
</blockquote>
<hr />
<h2>Backward Stepwise Selection</h2>
<div class="img">
<img src="https://d2mxuefqeaa7sj.cloudfront.net/s_675D8EAEE35D72F3DF626C18BB07F441157F58865CAEC85F97D5D9051B1B596F_1486697066225_+2017-02-10+11.24.14.png
" width="80%" height="80%" />       
</div>

<blockquote>
<p>限制： n 必須大於 p （否則沒辦法計算 最小平方法）  </p>
</blockquote>
<hr />
<h1>Choosing Optimal Model</h1>
<h2>Indirect estimate of testing error</h2>
<ul>
<li>$C_p = \frac{1}{n}(RSS+2d\hat{\sigma}^2)$</li>
</ul>
<blockquote>
<p>n: observation, d: numbers of predictors, $\hat{\sigma}^2$: estimate of the variance of the error</p>
</blockquote>
<ul>
<li>$AIC = \frac{1}{n\hat{\sigma}^2}(RSS+2d\hat{\sigma}^2) = -2logL+2\cdot d$</li>
</ul>
<blockquote>
<p>L: maximum likelihood estimate; 右式為一般 的 AIC通則+，左式則是線性迴歸特例公式</p>
</blockquote>
<ul>
<li>$BIC=\frac{1}{n}(RSS+log(n)d\hat{\sigma}^2)$</li>
</ul>
<blockquote>
<p>當 n &gt; 7 時，對於 predictor 較多的 model 懲罰更大。使用 BIC 會更偏好參數少的 model</p>
</blockquote>
<ul>
<li>$\text{Adjusted } R^2 = 1- \frac{RSS/(n-d-1)}{TSS/(n-1)}$</li>
</ul>
<hr />
<p>Direct estimate of testing error</p>
<ul>
<li>Validation</li>
<li>Cross Validation</li>
</ul>
<div class="img">
<img src="https://d2mxuefqeaa7sj.cloudfront.net/s_675D8EAEE35D72F3DF626C18BB07F441157F58865CAEC85F97D5D9051B1B596F_1486696080537_+2017-02-10+11.06.56.png
" width="80%" height="80%" />       
</div>

<h1>Shrinkage Methods</h1>
<ul>
<li>Ridge regression</li>
<li>Lasso</li>
</ul>
<p>Recall least square fitting procedure, RSS: </p>
<p>$RSS = \sum_{i=1}^{n}(y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij})^2$</p>
<p>Ridge regression:</p>
<p>$RSS + \lambda\sum_{j=1}^{p}\beta_{j}^2$</p>
<blockquote>
<p>$\lambda\sum_{j=1}^{p}\beta_{j}^2$： shrinkage penalty  </p>
<p>$\lambda$： tunning parameter（ $\lambda = 0$ 為 ordinary least square fitting, $\lambda$ 越大，係數會越小，逐漸逼近0）</p>
</blockquote>
<div class="img">
<img src="https://d2mxuefqeaa7sj.cloudfront.net/s_675D8EAEE35D72F3DF626C18BB07F441157F58865CAEC85F97D5D9051B1B596F_1487835450668_+2017-02-23+3.37.15.png
" width="80%" height="80%" />       
</div>

<blockquote>
<p>$\hat{\beta}$: least squares coefficient estimates  </p>
<p>$\hat{\beta}_{\lambda}^{R}$: ridge regression coefficient  </p>
<p>$\vert\vert\beta\vert\vert_2 =\sqrt{\sum_{j=1}^{p}\beta_{j}^2}$   </p>
</blockquote>
<p>LASSO:</p>
<p>$RSS + \lambda\sum_{j=1}^{p}|\beta_j|$</p>
<blockquote>
<p>ridge regression 的變形 (1996 invented)
當 $\lambda$ 夠大，可以讓係數變成0 而不是趨近於0 (<strong>sparsity</strong>)</p>
</blockquote>
<div class="img">
<img src="https://d2mxuefqeaa7sj.cloudfront.net/s_675D8EAEE35D72F3DF626C18BB07F441157F58865CAEC85F97D5D9051B1B596F_1487837715966_+2017-02-23+4.15.07.png
" width="80%" height="80%" />       
</div>

<h2>Issue of scale equivariant</h2>
<p>Scale equivariant: 已知 $X_i$ 利用最小平方法所得出的係數為 $\beta_i$。 若 $X_i$ 乘上定值 $c$ ，則得到的係數 $\hat{\beta_i}$ 會變成原本的 $1/c$ 倍。即 $X_i\beta_i$ 會是定值。
但在 Ridge regression 或是 LASSO 沒有這樣的特性，因此最好的做法，是在對 predictors 事前的標準化：  </p>
<p>$\tilde{x_{ij}}=\frac{x_{ij}}{\sqrt{\frac{1}{n}\sum_{i=1}^n(x_{ij}-\bar{x}_j)^2}}$</p>
<h2>Trade-off between Ridge regression and Least squares</h2>
<p>Bias-Variance trade-off:  </p>
<ul>
<li>least squares: high variance and low bias (complex model)  </li>
<li>ridge regression: high bias and low variance (simple model)</li>
</ul>
<div class="img">
<img src="https://d2mxuefqeaa7sj.cloudfront.net/s_675D8EAEE35D72F3DF626C18BB07F441157F58865CAEC85F97D5D9051B1B596F_1487836778224_+2017-02-23+3.59.28.png
" width="80%" height="80%" />       
</div>

<h2>Visualize the variable selection property</h2>
<p>Another formula for the LASSO and ridge regression:</p>
<div class="img">
<img src="https://d2mxuefqeaa7sj.cloudfront.net/s_675D8EAEE35D72F3DF626C18BB07F441157F58865CAEC85F97D5D9051B1B596F_1487837823987_+2017-02-23+4.16.49.png
" width="80%" height="80%" />       
</div>

<div class="img">
<img src="https://d2mxuefqeaa7sj.cloudfront.net/s_675D8EAEE35D72F3DF626C18BB07F441157F58865CAEC85F97D5D9051B1B596F_1487837893419_+2017-02-23+4.18.00.png
" width="80%" height="80%" />       
</div>

<h2>如何選擇 ridge regression 或 LASSO?</h2>
<p>dense model (gene) → ridge regression
sparse model (gender) → LASSO</p>
<h2>What’s about choosing the $\lambda$ (Tuning parameter) ?</h2>
<p><strong>Cross Validation:</strong></p>
<ol>
<li>資料先分成訓練、測試集</li>
<li>訓練集建模 (ridge or LASSO)</li>
<li>訓練模型含有若干 $\lambda$ 值</li>
<li>將不同 $\lambda$ 所得的模型預測結果與測試集 得到 RMSE</li>
<li>找出 min(RMSE) 的 $\lambda$ 值，該模型的係數</li>
</ol>
<p>code demo</p>
<div class="highlight"><pre><span></span><span class="kn">library</span><span class="p">(</span>leaps<span class="p">)</span>
<span class="kn">library</span><span class="p">(</span>ISLR<span class="p">)</span>
Hitters <span class="o">&lt;-</span> na.omit<span class="p">(</span>Hitters<span class="p">)</span>
trainNum <span class="o">&lt;-</span> <span class="kp">round</span><span class="p">(</span><span class="kp">dim</span><span class="p">(</span>Hitters<span class="p">)[</span><span class="m">1</span><span class="p">]</span><span class="o">*</span><span class="m">2</span><span class="o">/</span><span class="m">3</span><span class="p">)</span>
<span class="kp">set.seed</span><span class="p">(</span><span class="m">1</span><span class="p">)</span>
trainIdx <span class="o">&lt;-</span> <span class="kp">sample</span><span class="p">(</span><span class="kp">dim</span><span class="p">(</span>Hitters<span class="p">)[</span><span class="m">1</span><span class="p">],</span>trainNum<span class="p">,</span>replace<span class="o">=</span><span class="bp">F</span><span class="p">)</span>
<span class="c1"># training model (use the best lambda to get coef)</span>
lasso.tr <span class="o">&lt;-</span> glmnet<span class="p">(</span>x<span class="p">[</span>trainIdx<span class="p">,],</span>y<span class="p">[</span>trainIdx<span class="p">])</span>
lasso.tr <span class="c1"># 88 values of lambda</span>
<span class="c1"># cross validation?</span>
pred <span class="o">&lt;-</span> predict<span class="p">(</span>lasso.tr<span class="p">,</span>x<span class="p">[</span><span class="o">-</span>trainIdx<span class="p">,])</span>
<span class="kp">dim</span><span class="p">(</span>pred<span class="p">)</span> <span class="c1"># compute 92 times</span>
rmse <span class="o">&lt;-</span> <span class="kp">sqrt</span><span class="p">(</span><span class="kp">apply</span><span class="p">((</span>y<span class="p">[</span><span class="o">-</span>trainIdx<span class="p">]</span><span class="o">-</span>pred<span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="p">,</span><span class="m">2</span><span class="p">,</span><span class="kp">mean</span><span class="p">))</span>
plot<span class="p">(</span><span class="kp">log</span><span class="p">(</span>lasso.tr<span class="o">$</span>lambda<span class="p">),</span>rmse<span class="p">,</span>type<span class="o">=</span><span class="s">&quot;b&quot;</span><span class="p">,</span>xlab<span class="o">=</span><span class="s">&quot;log(lambda)&quot;</span><span class="p">)</span>
lam.best <span class="o">&lt;-</span> lasso.tr<span class="o">$</span>lambda<span class="p">[</span><span class="kp">order</span><span class="p">(</span>rmse<span class="p">)[</span><span class="m">1</span><span class="p">]]</span>
coef<span class="p">(</span>lasso.tr<span class="p">,</span>s<span class="o">=</span> lam.best<span class="p">)</span> <span class="c1"># after obtained the training model, we can set a lambda value which is got from the above method and finally got the coef</span>
</pre></div>


<hr />
<h1>Dimension Reduction Methods</h1>
<p>概念：利用原始自變量的線性轉換所生成新的自變量
原理：利用自變量間的相關，聚合新的自變量，減少共線性
目的：將原本的 $p$ 個變量 縮減到 $m$ 個變量, $m &lt; p$</p>
<div class="img">
<img src="https://d2mxuefqeaa7sj.cloudfront.net/s_675D8EAEE35D72F3DF626C18BB07F441157F58865CAEC85F97D5D9051B1B596F_1487839208963_+2017-02-23+4.39.52.png
" width="80%" height="80%" />       
</div>

<h2>Principle Component Regression:</h2>
<p>第一主成份：統計學家常用的假設，變異最大的方向向量（high variance is probably going to be associated with the response）</p>
<div class="img">
<img src="https://d2mxuefqeaa7sj.cloudfront.net/s_675D8EAEE35D72F3DF626C18BB07F441157F58865CAEC85F97D5D9051B1B596F_1487839331978_+2017-02-23+4.41.57.png
" width="80%" height="80%" />       
</div>

<p>缺點: 只考慮自變量的線性轉換，未考慮自變量與應變量間的關係，導致模型可能出現 full model is the best model 的狀況，無法化簡主成份模型應有的效果：</p>
<div class="img">
<img src="https://d2mxuefqeaa7sj.cloudfront.net/s_675D8EAEE35D72F3DF626C18BB07F441157F58865CAEC85F97D5D9051B1B596F_1487839631957_+2017-02-23+4.46.59.png
" width="80%" height="80%" />       
</div>

<h2>Partial Least Squares</h2>
<p>根據 PCR 得到的 $Z_1,...,Z_m$ 和 $y$ 的關係，篩選出 subset $Z_j$
屬於 supervised learning procedure</p>
<div id="disqus_thread"></div>

<script>
    /**
     *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
     *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
     */
    /*
    var disqus_config = function () {
        this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() {  // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');

        s.src = '//aady5566github.disqus.com/embed.js';

        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>

		<div id="article_meta">
				Category:
					<a href="/category/stat.html">Stat</a>
				<br />Tags:
					<a href="/tag/stat.html">Stat</a>
		</div>
	</article>

	<footer>
		<a href="/" class="button_accent">&larr;&nbsp;&nbsp;&nbsp;Back to blog</a>
	</footer>


	</section>

</body>
</html>